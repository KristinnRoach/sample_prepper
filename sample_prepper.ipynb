{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qy24vOpyvB-n"
      },
      "outputs": [],
      "source": [
        "# Install Dependencies\n",
        "\n",
        "# ffmpeg used under the hood for speed advantage:\n",
        "!apt-get update && apt-get install -y ffmpeg # -y libsndfile1\n",
        "# main audio processing packages:\n",
        "!pip install torchaudio librosa soundfile pydub\n",
        "# api setup and storage:\n",
        "!pip install flask flask-cors pyngrok firebase-admin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports\n",
        "from flask import Flask, request, jsonify, send_file\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials, storage\n",
        "\n",
        "from google.colab import userdata\n",
        "import tempfile\n",
        "import json\n",
        "import os\n",
        "import io\n",
        "import logging\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "from pathlib import Path\n",
        "import IPython.display as idp\n",
        "from IPython.core.magic import register_cell_magic\n",
        "import mimetypes\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import soundfile as sf\n",
        "import pydub\n",
        "import torchaudio\n",
        "import torch\n",
        "import torchaudio.transforms as T\n",
        "import torchaudio.functional as F\n",
        "import torch.nn.functional as FNN\n",
        "import librosa\n",
        "\n",
        "# For skipping cells\n",
        "@register_cell_magic\n",
        "def skip(line, cell):\n",
        "    return\n",
        "\n",
        "# Check if running on GPU\n",
        "if torch.cuda.is_available():\n",
        "  print(f\"GPU available: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")"
      ],
      "metadata": {
        "id": "D9LGwA0IZUvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Firebase Setup\n",
        "fire_cred = userdata.get('FB_CRED')\n",
        "\n",
        "# Only initialize if no apps exist\n",
        "if not firebase_admin._apps:\n",
        "    # Write the credentials to a temporary file\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix='.json') as temp_file:\n",
        "        temp_file.write(fire_cred.encode('utf-8'))\n",
        "        temp_file_path = temp_file.name\n",
        "\n",
        "    # Initialize Firebase with the temporary credential file\n",
        "    cred = credentials.Certificate(temp_file_path)\n",
        "    firebase_admin.initialize_app(cred, {\n",
        "        'storageBucket': 'sample-prep-dbd20.firebasestorage.app'\n",
        "    })\n",
        "\n",
        "    os.unlink(temp_file_path)\n",
        "\n",
        "bucket = storage.bucket()"
      ],
      "metadata": {
        "id": "QH5i14tqmqIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions\n",
        "\n",
        "def get_pitch_factor(original_pitch, target_pitch):\n",
        "    \"\"\"\n",
        "    Calculate the factor needed to transpose from original pitch to target pitch.\n",
        "    Returns a positive factor where:\n",
        "    - factor > 1 means transpose up\n",
        "    - factor < 1 means transpose down\n",
        "    \"\"\"\n",
        "    if original_pitch <= 0:\n",
        "        raise ValueError(\"Original pitch must be positive\")\n",
        "    return target_pitch / original_pitch\n",
        "\n",
        "#Get the mime type of the audio\n",
        "def get_mime_type(path):\n",
        "    mime_type = mimetypes.guess_type(path)\n",
        "    print(f\"{path}: Mime type: {mime_type}\")\n",
        "    return mime_type"
      ],
      "metadata": {
        "id": "EUSl7ThksIgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process 1: Normalize\n",
        "def normalize(waveform):\n",
        "    \"\"\"Peak normalize audio to range [-1, 1]\"\"\"\n",
        "    input_peak = torch.abs(waveform).max()\n",
        "    print(f\"[NORMALIZE] Input peak amplitude: {input_peak}\")\n",
        "\n",
        "    if input_peak > 0:\n",
        "        normalized_waveform = waveform / input_peak\n",
        "        output_peak = torch.abs(normalized_waveform).max()\n",
        "        print(f\"[NORMALIZE] Output peak amplitude: {output_peak}\")\n",
        "        return normalized_waveform\n",
        "\n",
        "    return waveform"
      ],
      "metadata": {
        "id": "eXLqt6V3HyFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process 2: Trim Silence\n",
        "\n",
        "def trim_silence(waveform, threshold_db=-50.0, min_length_ms=50):\n",
        "    \"\"\"Trim silence from start and end of audio using RMS energy threshold\"\"\"\n",
        "    # Convert to mono if stereo\n",
        "    if waveform.size(0) > 1:\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    # Calculate RMS energy in decibels using Torchs NN module\n",
        "    frame_length = int(min_length_ms * waveform.size(1) / 1000)\n",
        "    rms = torch.sqrt(FNN.avg_pool1d(waveform ** 2, kernel_size=frame_length, stride=1))\n",
        "    db = 20 * torch.log10(rms + 1e-8)\n",
        "\n",
        "    # Find start and end points above threshold\n",
        "    mask = (db > threshold_db)[0]\n",
        "    nonzero = torch.nonzero(mask)\n",
        "\n",
        "    if len(nonzero) == 0:\n",
        "        return waveform\n",
        "\n",
        "    start, end = nonzero[0], nonzero[-1]\n",
        "    return waveform[:, start:end+1]\n"
      ],
      "metadata": {
        "id": "E1MnfT4kHu1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process 3: Pitch Detection: currently using librosa PYIN algorithm\n",
        "\n",
        "# needed for subprocess ?\n",
        "import subprocess\n",
        "\n",
        "def get_main_pitch(audio_path, min_note='C1', max_note='C7'):\n",
        "    \"\"\"\n",
        "    Get the main pitch of an audio file with robust format handling.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    audio_path : str or Path\n",
        "        Path to the audio file\n",
        "    min_note : str\n",
        "        Lowest note to consider (in scientific pitch notation)\n",
        "    max_note : str\n",
        "        Highest note to consider (in scientific pitch notation)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (frequency in Hz, note name, confidence score)\n",
        "    \"\"\"\n",
        "    audio_path = Path(audio_path)\n",
        "\n",
        "    # Get file extension and mime type\n",
        "    ext = audio_path.suffix.lower()\n",
        "\n",
        "    try:\n",
        "        # Create a temporary directory for processing\n",
        "        with tempfile.TemporaryDirectory() as temp_dir:\n",
        "            temp_dir = Path(temp_dir)\n",
        "            temp_wav = temp_dir / 'temp.wav'\n",
        "\n",
        "            # Convert to WAV if needed\n",
        "            if ext == '.webm':\n",
        "                logger.info(f\"Converting WEBM to WAV: {audio_path}\")\n",
        "                subprocess.run([\n",
        "                    'ffmpeg', '-i', str(audio_path),\n",
        "                    '-acodec', 'pcm_s16le',\n",
        "                    '-ar', '44100',\n",
        "                    str(temp_wav)\n",
        "                ], check=True, capture_output=True)\n",
        "                processing_path = temp_wav\n",
        "            else:\n",
        "                processing_path = audio_path\n",
        "\n",
        "            # Load audio with explicit parameters\n",
        "            y, sr = librosa.load(\n",
        "                processing_path,\n",
        "                sr=None,  # Keep original sample rate\n",
        "                mono=True  # Convert to mono if needed\n",
        "            )\n",
        "\n",
        "            logger.info(f\"Loaded audio: sample rate={sr}, duration={len(y)/sr:.2f}s\")\n",
        "\n",
        "            # Normalize audio if needed\n",
        "            if np.abs(y).max() > 1.0:\n",
        "                y = y / np.abs(y).max()\n",
        "\n",
        "            # Calculate pitch using PYIN algorithm\n",
        "            f0, voiced_flag, voiced_probs = librosa.pyin(\n",
        "                y,\n",
        "                fmin=librosa.note_to_hz(min_note),\n",
        "                fmax=librosa.note_to_hz(max_note),\n",
        "                sr=sr,\n",
        "                frame_length=2048,  # Increase for better low frequency resolution\n",
        "                win_length=1024,\n",
        "                hop_length=512\n",
        "            )\n",
        "\n",
        "            # Filter out unvoiced and low probability segments\n",
        "            mask = voiced_flag & (voiced_probs > 0.6)\n",
        "            f0_valid = f0[mask]\n",
        "\n",
        "            if len(f0_valid) == 0:\n",
        "                logger.warning(\"No valid pitch detected\")\n",
        "                return None, None, 0.0\n",
        "\n",
        "            # Get the median frequency\n",
        "            median_f0 = float(np.median(f0_valid))\n",
        "\n",
        "            # Convert median frequency to note\n",
        "            closest_note = librosa.hz_to_note(median_f0)\n",
        "            note_freq = librosa.note_to_hz(closest_note)\n",
        "            note = {'closest_note': closest_note, 'freq': note_freq}\n",
        "\n",
        "            # Calculate confidence score\n",
        "            #voiced_probs = voiced_probs[mask]\n",
        "            confidence = float(np.mean(voiced_probs[voiced_flag]))\n",
        "\n",
        "            # Log results\n",
        "            print_pitch_details(audio_path, median_f0, note,  confidence)\n",
        "\n",
        "            return median_f0, note, confidence\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logger.error(f\"FFmpeg conversion failed: {e.stderr.decode()}\")\n",
        "        raise RuntimeError(\"Audio conversion failed\") from e\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Pitch detection failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Utility for logging and debugging pitch detection\n",
        "def print_pitch_details(audio_path, freq, note, conf):\n",
        "    \"\"\"\n",
        "    Test the pitch detection on an audio file and print detailed results.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if freq is not None:\n",
        "            print(f\"\\nPitch detection results for {audio_path}:\")\n",
        "            print(f\"Main frequency: {freq:.4f} Hz\")\n",
        "            print(f\"Closest musical note is: {note['closest_note']}, with frequency ({note['freq']:.4f} Hz)\")\n",
        "            print(f\"Confidence: {conf:.3f}\")\n",
        "        else:\n",
        "            print(f\"\\nNo valid pitch detected in {audio_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError processing {audio_path}: {str(e)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "BSTuksfu1BsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process 4: Transposition\n",
        "\n",
        "def transpose(waveform: torch.Tensor,\n",
        "             sample_rate: int,\n",
        "             factor: float) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Resamples audio it by a given factor.\n",
        "\n",
        "    Args:\n",
        "        waveform (torch.Tensor): Input audio waveform tensor\n",
        "        sample_rate (int): Original sample rate of the audio\n",
        "        factor (float): Factor to transpose by:\n",
        "        (e.g., 0.5 for one octave down, 2.0 for one octave up)\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Transposed waveform\n",
        "        Should be played back at original sample rate for transposing effect\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If factor is <= 0\n",
        "        TypeError: If inputs are of incorrect type\n",
        "    \"\"\"\n",
        "    # Input validation\n",
        "    if not isinstance(factor, (int, float)):\n",
        "        raise TypeError(\"Factor must be a number\")\n",
        "    if factor <= 0:\n",
        "        raise ValueError(\"Factor must be positive\")\n",
        "    if not isinstance(sample_rate, int):\n",
        "        raise TypeError(\"Sample rate must be an integer\")\n",
        "    if not torch.is_tensor(waveform):\n",
        "        raise TypeError(\"Waveform must be a torch.Tensor\")\n",
        "\n",
        "    # Calculate new sample rate\n",
        "    resample_rate = int(sample_rate * factor)\n",
        "\n",
        "    # Create resampler\n",
        "    resampler = T.Resample(\n",
        "        orig_freq=sample_rate,\n",
        "        new_freq=resample_rate,\n",
        "        dtype=waveform.dtype\n",
        "    )\n",
        "\n",
        "    # Perform resampling\n",
        "    resampled_waveform = resampler(waveform)\n",
        "\n",
        "    return resampled_waveform\n"
      ],
      "metadata": {
        "id": "mN-IHY5edguX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Audio Processing Pipeline entry point\n",
        "\n",
        "def process_audio_file(file_path, options, sample_rate=None):\n",
        "    \"\"\"\n",
        "    Process audio file according to specified options.\n",
        "\n",
        "    Args:\n",
        "        file_path (Path): Path to input audio file\n",
        "        options (dict): Processing options including:\n",
        "            - normalize (bool): Whether to normalize audio\n",
        "            - trim (bool): Whether to trim silence\n",
        "            - tune (bool): Whether to apply pitch tuning\n",
        "            - target_pitch (float): Target pitch frequency if tuning\n",
        "        sample_rate (int, optional): Target sample rate for processing\n",
        "\n",
        "    Returns:\n",
        "        tuple: (processed_waveform, sample_rate)\n",
        "    \"\"\"\n",
        "    # Load audio\n",
        "    waveform, sr = torchaudio.load(file_path)\n",
        "    waveform = waveform.to(device)\n",
        "\n",
        "    # Resample if needed\n",
        "    if sample_rate and sample_rate != sr:\n",
        "        resampler = torchaudio.transforms.Resample(sr, sample_rate)\n",
        "        waveform = resampler(waveform)\n",
        "        sr = sample_rate\n",
        "\n",
        "    # Apply processing based on options\n",
        "    if options['normalize']:\n",
        "        waveform = normalize(waveform)\n",
        "\n",
        "    if options['trim']:\n",
        "        waveform = trim_silence(waveform)\n",
        "\n",
        "    if options['tune']:\n",
        "        detected_pitch, note, confidence = get_main_pitch(file_path)\n",
        "        C4_FREQ = 261.6255 # default\n",
        "        target = options.get('target_pitch', C4_FREQ)  # C4 frequency\n",
        "        print(target)\n",
        "        factor = get_pitch_factor(detected_pitch, target)\n",
        "        print(factor)\n",
        "        waveform = transpose(waveform, sr, factor=factor)\n",
        "\n",
        "    return waveform, sr"
      ],
      "metadata": {
        "id": "KwXrZN0285Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API Setup\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Create fixed directories in Colab's content directory\n",
        "BASE_DIR = Path('/content')  # Colab's base content directory\n",
        "UPLOAD_DIR = BASE_DIR / 'uploads'\n",
        "OUTPUT_DIR = BASE_DIR / 'outputs'\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Upload directory created at: {UPLOAD_DIR}\")\n",
        "print(f\"Output directory created at: {OUTPUT_DIR}\")\n",
        "\n",
        "\n",
        "# Setup logger\n",
        "logger = logging.getLogger(__name__) # ('werkzeug')\n",
        "logger.setLevel(logging.ERROR)\n",
        "logger.info(f\"Upload directory: {UPLOAD_DIR}\")\n",
        "\n",
        "# Cell 7: Process Endpoint\n",
        "@app.route('/process', methods=['POST'])\n",
        "def process_audio():\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({'error': 'No file provided'}), 400\n",
        "\n",
        "    # Get and validate options\n",
        "    options = {\n",
        "        'normalize': True,\n",
        "        'trim': True,\n",
        "        'tune': True,\n",
        "        'saveToFirebase': False,\n",
        "        'returnType': 'blob'\n",
        "    }\n",
        "\n",
        "    if 'options' in request.form:\n",
        "        try:\n",
        "            user_options = json.loads(request.form['options'])\n",
        "            options.update(user_options)\n",
        "        except json.JSONDecodeError:\n",
        "            return jsonify({'error': 'Invalid options format'}), 400\n",
        "\n",
        "    file = request.files['file']\n",
        "\n",
        "    # Absolute paths with sanitized filenames\n",
        "    safe_filename = os.path.basename(file.filename)\n",
        "    input_path = UPLOAD_DIR / safe_filename\n",
        "    output_path = OUTPUT_DIR / f\"processed_{safe_filename}\"\n",
        "\n",
        "    # Save uploaded file\n",
        "    file.save(str(input_path))\n",
        "\n",
        "    try:\n",
        "\n",
        "        # Process audio through pipeline\n",
        "        waveform, sample_rate = process_audio_file(str(input_path), options)\n",
        "\n",
        "        # Save processed audio\n",
        "        output_path = UPLOAD_DIR / f\"processed_{file.filename}\"\n",
        "        outpath_str = str(output_path)\n",
        "        torchaudio.save(outpath_str, waveform, sample_rate, format=\"wav\") # wav for testing\n",
        "\n",
        "        # Handle Firebase storage if requested\n",
        "        if options['saveToFirebase']:\n",
        "            blob = bucket.blob(f'processed_{file.filename}')\n",
        "            blob.upload_from_filename(outpath_str)\n",
        "\n",
        "            if options['returnType'] == 'url':\n",
        "                download_url = blob.generate_signed_url(expiration=300)\n",
        "                return jsonify({'download_url': download_url})\n",
        "\n",
        "        # Return audio file directly if no Firebase or URL not requested\n",
        "        if options['returnType'] == 'blob' or not options['saveToFirebase']:\n",
        "            return send_file(output_path)\n",
        "\n",
        "        return send_file(output_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Processing failed: {str(e)}\")\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "    finally:\n",
        "        #temp_path.unlink(missing_ok=True)\n",
        "        Path(input_path).unlink(missing_ok=True)\n",
        "        Path(output_path).unlink(missing_ok=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "8z2f_aqAICVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Run the Server\n",
        "\n",
        "ngrok.set_auth_token(userdata.get('NGROK_SECRET'))\n",
        "public_url = ngrok.connect(5000, domain=\"singular-roughy-humane.ngrok-free.app\")\n",
        "print(f' * Public URL: {public_url}')\n",
        "\n",
        "# Setup logger\n",
        "logger = logging.getLogger(__name__) # ('werkzeug')\n",
        "logger.setLevel(logging.ERROR)\n",
        "\n",
        "# Start server\n",
        "app.run(port=5000)"
      ],
      "metadata": {
        "id": "14j79XyeIFVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%skip\n",
        "\n",
        "# Test cell: Normalization\n",
        "\n",
        "# Test case 1: High amplitude signal\n",
        "print(\"Test 1: High amplitude signal\")\n",
        "high_amp = torch.tensor([[2.5, -3.0, 1.5]])\n",
        "normalized_high = normalize(high_amp)\n",
        "print()\n",
        "\n",
        "# Test case 2: Normal amplitude signal\n",
        "print(\"Test 2: Normal amplitude signal\")\n",
        "normal_amp = torch.tensor([[0.5, -0.7, 0.3]])\n",
        "normalized_normal = normalize(normal_amp)\n",
        "print()\n",
        "\n",
        "# Test case 3: Zero signal\n",
        "print(\"Test 3: Zero signal\")\n",
        "zero_amp = torch.zeros((1, 3))\n",
        "normalized_zero = normalize(zero_amp)\n",
        "print()\n",
        "\n",
        "# Test case 4: Predominantly negative signal\n",
        "print(\"Test 4: Predominantly negative signal\")\n",
        "neg_amp = torch.tensor([[-2.5, 0.5, -3.0]])\n",
        "normalized_neg = normalize(neg_amp)\n",
        "print()\n",
        "\n",
        "# Test case 5: Multi-channel audio\n",
        "print(\"Test 5: Stereo signal\")\n",
        "stereo_amp = torch.tensor([[2.0, -3.0, 1.0],\n",
        "                          [1.5, -2.0, 0.5]])\n",
        "normalized_stereo = normalize(stereo_amp)\n",
        "print()\n",
        "\n",
        "# Test case 6: Different precision\n",
        "print(\"Test 6: Float64 signal\")\n",
        "float64_amp = torch.tensor([[2.5, -3.0, 1.5]], dtype=torch.float64)\n",
        "normalized_float64 = normalize(float64_amp)\n",
        "print()\n",
        "\n",
        "# Display results\n",
        "print(\"Final Results:\")\n",
        "for name, orig, norm in [\n",
        "    (\"High amplitude\", high_amp, normalized_high),\n",
        "    (\"Normal amplitude\", normal_amp, normalized_normal),\n",
        "    (\"Zero amplitude\", zero_amp, normalized_zero),\n",
        "    (\"Negative amplitude\", neg_amp, normalized_neg),\n",
        "    (\"Stereo\", stereo_amp, normalized_stereo),\n",
        "    (\"Float64\", float64_amp, normalized_float64)\n",
        "]:\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"Original: {orig}\")\n",
        "    print(f\"Normalized: {norm}\")\n",
        "    if norm is not None:\n",
        "        print(f\"Max abs value: {torch.abs(norm).max()}\")"
      ],
      "metadata": {
        "id": "ArfJdG05Miq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%skip\n",
        "\n",
        "# Test Cell: Silence Trimming\n",
        "\n",
        "def generate_test_audio_with_middle_silence(sample_rate=16000, duration_seconds=3):\n",
        "    \"\"\"Generate test audio with silences at start, middle, and end\"\"\"\n",
        "    total_samples = int(sample_rate * duration_seconds)\n",
        "    silence_samples = int(sample_rate * 0.5)  # 0.5 seconds of silence\n",
        "\n",
        "    # Create the waveform\n",
        "    waveform = torch.zeros(1, total_samples)\n",
        "\n",
        "    # Generate two sine waves for the first and third thirds\n",
        "    t1 = torch.linspace(0, duration_seconds/3, total_samples//3 - silence_samples)\n",
        "    t2 = torch.linspace(0, duration_seconds/3, total_samples//3 - silence_samples)\n",
        "    frequency = 440  # A4 note\n",
        "    signal1 = 0.5 * torch.sin(2 * math.pi * frequency * t1)\n",
        "    signal2 = 0.5 * torch.sin(2 * math.pi * frequency * t2)\n",
        "\n",
        "    # Insert the signals with silence at start, middle, and end\n",
        "    mid_point = total_samples // 2\n",
        "    waveform[0, silence_samples:total_samples//3] = signal1\n",
        "    waveform[0, -total_samples//3:-silence_samples] = signal2\n",
        "\n",
        "    return waveform, sample_rate\n",
        "\n",
        "def plot_waveforms_with_db(waveform, processed_waveform, db, threshold_db, sample_rate):\n",
        "    \"\"\"Plot original, processed waveforms, and the dB levels\"\"\"\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 12))\n",
        "\n",
        "    # Plot original\n",
        "    time_orig = torch.arange(waveform.size(1)) / sample_rate\n",
        "    ax1.plot(time_orig, waveform[0])\n",
        "    ax1.set_title('Original Waveform')\n",
        "    ax1.set_ylabel('Amplitude')\n",
        "\n",
        "    # Plot processed\n",
        "    time_proc = torch.arange(processed_waveform.size(1)) / sample_rate\n",
        "    ax2.plot(time_proc, processed_waveform[0])\n",
        "    ax2.set_title('Processed Waveform')\n",
        "    ax2.set_ylabel('Amplitude')\n",
        "\n",
        "    # Plot dB levels\n",
        "    time_db = torch.arange(len(db)) / (len(db) / duration_seconds)\n",
        "    ax3.plot(time_db, db)\n",
        "    ax3.axhline(y=threshold_db, color='r', linestyle='--', label=f'Threshold ({threshold_db} dB)')\n",
        "    ax3.set_title('RMS Energy (dB)')\n",
        "    ax3.set_xlabel('Time (s)')\n",
        "    ax3.set_ylabel('dB')\n",
        "    ax3.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate test audio with middle silence\n",
        "duration_seconds = 3\n",
        "waveform, sample_rate = generate_test_audio_with_middle_silence(duration_seconds=duration_seconds)\n",
        "\n",
        "# Process the audio and capture intermediate values\n",
        "threshold_db = -50.0\n",
        "min_length_ms = 50\n",
        "\n",
        "# Calculate RMS energy in decibels (copying from trim_silence function)\n",
        "if waveform.size(0) > 1:\n",
        "    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "frame_length = int(min_length_ms * waveform.size(1) / 1000)\n",
        "rms = torch.sqrt(F.avg_pool1d(waveform ** 2, kernel_size=frame_length, stride=1))\n",
        "db = 20 * torch.log10(rms + 1e-8)[0]\n",
        "\n",
        "# Process audio\n",
        "start_time = time.time()\n",
        "processed_waveform = trim_silence(waveform, threshold_db, min_length_ms)\n",
        "processing_time = time.time() - start_time\n",
        "\n",
        "# Plot with dB levels\n",
        "plot_waveforms_with_db(waveform, processed_waveform, db, threshold_db, sample_rate)\n",
        "\n",
        "# Print analysis\n",
        "original_duration = waveform.size(1) / sample_rate\n",
        "processed_duration = processed_waveform.size(1) / sample_rate\n",
        "print(f\"\\nDuration Analysis:\")\n",
        "print(f\"Original audio duration: {original_duration:.2f} seconds\")\n",
        "print(f\"Processed audio duration: {processed_duration:.2f} seconds\")\n",
        "print(f\"\\nProcessing time: {processing_time*1000:.2f} ms\")\n",
        "\n",
        "# Play the audio\n",
        "print(\"\\nPlaying original audio:\")\n",
        "ipd.display(ipd.Audio(waveform.numpy(), rate=sample_rate))\n",
        "print(\"\\nPlaying processed audio:\")\n",
        "ipd.display(ipd.Audio(processed_waveform.numpy(), rate=sample_rate))"
      ],
      "metadata": {
        "id": "-154advcLKkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%skip\n",
        "\n",
        "# Test Cell: Pitch Detection\n",
        "\n",
        "\n",
        "def generate_sine_wave(frequency, duration=1.0, sample_rate=44100, amplitude=0.5):\n",
        "    \"\"\"Generate a sine wave for testing.\n",
        "\n",
        "    Args:\n",
        "        frequency (float): The frequency of the sine wave in Hz\n",
        "        duration (float): Duration of the signal in seconds\n",
        "        sample_rate (int): Sample rate in Hz\n",
        "        amplitude (float): Amplitude of the signal\n",
        "\n",
        "    Returns:\n",
        "        tuple: (samples, sample_rate)\n",
        "    \"\"\"\n",
        "    t = np.linspace(0, duration, int(sample_rate * duration))\n",
        "    samples = amplitude * np.sin(2 * np.pi * frequency * t)\n",
        "    return samples, sample_rate  # Return both the samples and sample rate\n",
        "\n",
        "\n",
        "\n",
        "def test_pitch_detection():\n",
        "    # Create temporary directory for test files\n",
        "    temp_dir = Path(tempfile.mkdtemp())\n",
        "\n",
        "    # Test frequencies (in Hz)\n",
        "    test_frequencies = [\n",
        "        261.63,  # C4\n",
        "        293.66,  # D4\n",
        "        329.63,  # E4\n",
        "        349.23,  # F4\n",
        "        392.00,  # G4\n",
        "        440.00,  # A4\n",
        "        493.88,  # B4\n",
        "        523.25   # C5\n",
        "    ]\n",
        "\n",
        "    note_names = ['C4', 'D4', 'E4', 'F4', 'G4', 'A4', 'B4', 'C5']\n",
        "    results = []\n",
        "    execution_times = []\n",
        "\n",
        "    print(\"Starting pitch detection test...\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    try:\n",
        "        # Test synthetic tones\n",
        "        for freq, note in zip(test_frequencies, note_names):\n",
        "            # Generate and save test tone\n",
        "            samples, sample_rate = generate_sine_wave(freq)\n",
        "            temp_file = temp_dir / f\"{note}_{freq}Hz.wav\"\n",
        "            sf.write(temp_file, samples, sample_rate)\n",
        "\n",
        "            # Time the pitch detection\n",
        "            start_time = time.time()\n",
        "            detected_freq = librosa_get_main_pitch(str(temp_file))\n",
        "            end_time = time.time()\n",
        "            execution_times.append(end_time - start_time)\n",
        "\n",
        "            if detected_freq is not None:\n",
        "                # Calculate error\n",
        "                error_cents = 1200 * np.log2(detected_freq / freq)\n",
        "                error_hz = detected_freq - freq\n",
        "\n",
        "                results.append({\n",
        "                    'note': note,\n",
        "                    'expected_freq': freq,\n",
        "                    'detected_freq': detected_freq,\n",
        "                    'error_hz': error_hz,\n",
        "                    'error_cents': error_cents\n",
        "                })\n",
        "\n",
        "                print(f\"Note: {note}\")\n",
        "                print(f\"Expected: {freq:.1f} Hz\")\n",
        "                print(f\"Detected: {detected_freq:.1f} Hz\")\n",
        "                print(f\"Error: {error_hz:.1f} Hz ({error_cents:.1f} cents)\")\n",
        "                print(f\"Execution time: {(end_time - start_time):.3f} seconds\")\n",
        "                print(\"-\" * 50)\n",
        "            else:\n",
        "                print(f\"Warning: No pitch detected for {note}\")\n",
        "                print(\"-\" * 50)\n",
        "\n",
        "        # Print timing statistics\n",
        "        mean_time = np.mean(execution_times)\n",
        "        std_time = np.std(execution_times)\n",
        "        print(f\"\\nTiming Statistics:\")\n",
        "        print(f\"Mean execution time: {mean_time:.3f} seconds\")\n",
        "        print(f\"Std dev of execution time: {std_time:.3f} seconds\")\n",
        "\n",
        "\n",
        "    finally:\n",
        "        # Cleanup temporary files\n",
        "        for file in temp_dir.glob(\"*.wav\"):\n",
        "            try:\n",
        "                file.unlink()\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not delete {file}: {e}\")\n",
        "        try:\n",
        "            temp_dir.rmdir()\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not delete temp directory: {e}\")\n",
        "\n",
        "# Run the test\n",
        "if __name__ == \"__main__\":\n",
        "    test_pitch_detection()"
      ],
      "metadata": {
        "id": "Jqh9DHlrHkO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra test draft for pitch detection - ignore ##\n",
        "\n",
        "base = 'sPno_'\n",
        "note_list = ['C3', 'D3', 'E3', 'F3', 'G3', 'A3', 'B3', 'C4', 'D4', 'E4', 'F4', 'G4', 'A4', 'B4'] # [pno + 'C3', pno + 'D3', pno + 'E3', pno + 'F3', pno + 'G3', pno + 'A3', pno + 'B3', pno + 'C4', pno + 'D4', pno + 'E4', pno + 'F4', pno + 'G4', pno + 'A4', pno + 'B4']\n",
        "base_path = f'/content/{base}'\n",
        "extensions = ['webm']\n",
        "\n",
        "for note in note_list:\n",
        "  print(f\"Sample: {base}{note}\")\n",
        "  for ext in extensions:\n",
        "    sample_path = f'{base_path}{note}.{ext}'\n",
        "    mimetype = logMimeType(sample_path)\n",
        "\n",
        "    get_main_pitch(sample_path)"
      ],
      "metadata": {
        "id": "tn88Hg_3NaJw"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}