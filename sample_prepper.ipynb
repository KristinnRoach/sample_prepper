{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. First checking CUDA availability and PyTorch version\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Current PyTorch version: {torch.__version__}\")\n",
        "print(f\"Current CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'None'}\")\n"
      ],
      "metadata": {
        "id": "OawKkprFUrpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qy24vOpyvB-n"
      },
      "outputs": [],
      "source": [
        "# 2. Install Dependencies\n",
        "\n",
        "!pip install fastapi uvicorn librosa python-multipart ffmpeg-python aiofiles soundfile pyngrok nest_asyncio\n",
        "!apt-get install -y sox ffmpeg libportaudio2\n",
        "\n",
        "#!pip install fastapi==0.109.0 uvicorn==0.27.0 torch==2.1.2 torchaudio==2.1.2 librosa==0.10.1 numpy==1.26.3 python-multipart==0.0.6 ffmpeg-python python-multipart aiofiles soundfile pyngrok\n",
        "\n",
        "# ffmpeg used under the hood for speed advantage:\n",
        "#!apt-get update && apt-get install -y ffmpeg # -y libsndfile1\n",
        "# main audio processing packages:\n",
        "#!pip install torch torchaudio librosa numpy\n",
        "# api setup and storage:\n",
        "#!pip install flask flask-cors pyngrok firebase-admin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update && apt-get install -y sox libsox-dev\n",
        "!pip install torchaudio --no-cache-dir"
      ],
      "metadata": {
        "id": "P8gzBUDdcU4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. setup directories\n",
        "\n",
        "!mkdir -p /content/sample-prepper/uploads\n",
        "!mkdir -p /content/sample-prepper/output"
      ],
      "metadata": {
        "id": "vTpCkB59Y8Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Imports\n",
        "from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks, Form\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.responses import FileResponse\n",
        "import uvicorn\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from google.colab import userdata\n",
        "import logging\n",
        "import json\n",
        "import os\n",
        "import io\n",
        "import logging\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "from pathlib import Path\n",
        "import IPython.display as ipd\n",
        "from IPython.core.magic import register_cell_magic\n",
        "import mimetypes\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torchaudio.functional as F\n",
        "import librosa\n",
        "\n",
        "\n",
        "# For skipping cells\n",
        "@register_cell_magic\n",
        "def skip(line, cell):\n",
        "    return\n",
        "\n",
        "# Check if running on GPU\n",
        "if torch.cuda.is_available():\n",
        "  print(f\"GPU available: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")"
      ],
      "metadata": {
        "id": "xUCgFxGdT_UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logging setup\n",
        "\n",
        "def setup_minimal_logger():\n",
        "    \"\"\"Configure minimal logging for core processing steps\"\"\"\n",
        "    logger = logging.getLogger('audio')\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "    # Clear any existing handlers\n",
        "    logger.handlers = []\n",
        "\n",
        "    # Simple console handler\n",
        "    console = logging.StreamHandler()\n",
        "    console.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(message)s')\n",
        "    console.setFormatter(formatter)\n",
        "    logger.addHandler(console)\n",
        "\n",
        "    return logger\n",
        "\n",
        "# Initialize logger\n",
        "log = setup_minimal_logger()"
      ],
      "metadata": {
        "id": "zSPcgTWLuW8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Firebase Setup\n",
        "fire_cred = userdata.get('FB_CRED')\n",
        "\n",
        "# Only initialize if no apps exist\n",
        "if not firebase_admin._apps:\n",
        "    # Write the credentials to a temporary file\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix='.json') as temp_file:\n",
        "        temp_file.write(fire_cred.encode('utf-8'))\n",
        "        temp_file_path = temp_file.name\n",
        "\n",
        "    # Initialize Firebase with the temporary credential file\n",
        "    cred = credentials.Certificate(temp_file_path)\n",
        "    firebase_admin.initialize_app(cred, {\n",
        "        'storageBucket': 'sample-prep-dbd20.firebasestorage.app'\n",
        "    })\n",
        "\n",
        "    os.unlink(temp_file_path)\n",
        "\n",
        "bucket = storage.bucket()"
      ],
      "metadata": {
        "id": "QH5i14tqmqIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main.py\n",
        "\n",
        "# Create fixed directories\n",
        "BASE_DIR = Path('/content/sample-prepper')\n",
        "UPLOAD_DIR = BASE_DIR / 'uploads'\n",
        "OUTPUT_DIR = BASE_DIR\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "log.info(f\"Using device: {device}\")\n",
        "\n",
        "def get_mime_type(path: str) -> tuple:\n",
        "    \"\"\"Get the mime type of the audio file.\n",
        "\n",
        "    Args:\n",
        "        path (str): Path to the audio file\n",
        "\n",
        "    Returns:\n",
        "        tuple: Mime type and encoding\n",
        "    \"\"\"\n",
        "    mime_type = mimetypes.guess_type(path)\n",
        "    log.info(f\"{path}: Mime type: {mime_type}\")\n",
        "    return mime_type\n",
        "\n",
        "def get_output_format(input_format: str, requested_format: str = None) -> str:\n",
        "    \"\"\"Determine the output format based on input and requested format.\n",
        "\n",
        "    Args:\n",
        "        input_format (str): Original file format (e.g., 'mp3', 'webm')\n",
        "        requested_format (str): Optional requested output format\n",
        "\n",
        "    Returns:\n",
        "        str: Output format to use\n",
        "    \"\"\"\n",
        "    valid_formats = {'wav', 'mp3', 'webm'}\n",
        "\n",
        "    if requested_format and requested_format.lower() in valid_formats:\n",
        "        return requested_format.lower()\n",
        "\n",
        "    # Default to input format, or wav if input format not supported\n",
        "    return input_format.lower() if input_format.lower() in valid_formats else 'wav'\n",
        "\n",
        "\n",
        "def normalize(waveform: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Peak normalize audio to range [-1, 1].\n",
        "\n",
        "    Args:\n",
        "        waveform (torch.Tensor): Input audio waveform\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Normalized waveform\n",
        "    \"\"\"\n",
        "    input_peak = torch.abs(waveform).max()\n",
        "    log.info(f\"[NORMALIZE] Input peak amplitude: {input_peak}\")\n",
        "\n",
        "    if input_peak > 0:\n",
        "        normalized_waveform = waveform / input_peak\n",
        "        output_peak = torch.abs(normalized_waveform).max()\n",
        "        log.info(f\"[NORMALIZE] Output peak amplitude: {output_peak}\")\n",
        "        return normalized_waveform\n",
        "    return waveform\n",
        "\n",
        "def get_pitch_factor(original_pitch: float, target_pitch: float) -> float:\n",
        "    \"\"\"Calculate the factor needed to transpose from original pitch to target pitch.\n",
        "\n",
        "    Args:\n",
        "        original_pitch (float): Original pitch frequency in Hz\n",
        "        target_pitch (float): Target pitch frequency in Hz\n",
        "\n",
        "    Returns:\n",
        "        float: Pitch adjustment factor\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If original pitch is not positive\n",
        "    \"\"\"\n",
        "    if original_pitch <= 0:\n",
        "        raise ValueError(\"Original pitch must be positive\")\n",
        "    return target_pitch / original_pitch\n",
        "\n",
        "def transpose_torch(waveform: torch.Tensor, sample_rate: int, factor: float) -> torch.Tensor:\n",
        "    \"\"\"Transpose audio by resampling.\n",
        "\n",
        "    Args:\n",
        "        waveform (torch.Tensor): Input audio waveform\n",
        "        sample_rate (int): Original sample rate\n",
        "        factor (float): Pitch adjustment factor\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Transposed waveform\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If inputs are of wrong type\n",
        "        ValueError: If factor is not positive\n",
        "    \"\"\"\n",
        "    if not isinstance(factor, (int, float)):\n",
        "        raise TypeError(\"Factor must be a number\")\n",
        "    if factor <= 0:\n",
        "        raise ValueError(\"Factor must be positive\")\n",
        "    if not isinstance(sample_rate, int):\n",
        "        raise TypeError(\"Sample rate must be an integer\")\n",
        "    if not torch.is_tensor(waveform):\n",
        "        raise TypeError(\"Waveform must be a torch.Tensor\")\n",
        "\n",
        "    resample_rate = int(sample_rate / factor)\n",
        "    resampler = T.Resample(\n",
        "        orig_freq=sample_rate,\n",
        "        new_freq=resample_rate,\n",
        "        dtype=waveform.dtype\n",
        "    )\n",
        "\n",
        "    return resampler(waveform)\n",
        "\n",
        "def trim_silence(waveform: torch.Tensor, threshold_db: float = -50.0,\n",
        "                min_length_ms: float = 50, sr: int = 44100) -> torch.Tensor:\n",
        "    \"\"\"Trim silence from start and end of audio using numpy for fast processing.\n",
        "\n",
        "    Args:\n",
        "        waveform (torch.Tensor): Input audio waveform\n",
        "        threshold_db (float): Threshold in decibels below which audio is considered silence\n",
        "        min_length_ms (float): Minimum length of audio segment in milliseconds\n",
        "        sr (int): Sample rate of the audio\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Trimmed waveform\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Convert to numpy and ensure it's flat\n",
        "        audio_np = waveform.cpu().numpy()\n",
        "        if audio_np.ndim == 2:\n",
        "            audio_np = audio_np.squeeze(0)  # Remove channel dimension\n",
        "\n",
        "        # Calculate frame length\n",
        "        frame_length = int(min_length_ms * sr / 1000)\n",
        "\n",
        "        # Quick returns for short audio\n",
        "        if len(audio_np) < frame_length or len(audio_np) // frame_length == 0:\n",
        "            return waveform\n",
        "\n",
        "        # Reshape audio into frames (faster than processing sample by sample)\n",
        "        frames = audio_np[:len(audio_np) - (len(audio_np) % frame_length)]\n",
        "        frames = frames.reshape(-1, frame_length)\n",
        "\n",
        "        # Vectorized RMS energy calculation\n",
        "        rms = np.sqrt(np.mean(np.square(frames), axis=1))\n",
        "        db = 20 * np.log10(rms + 1e-8)\n",
        "\n",
        "        # Find start and end points above threshold\n",
        "        mask = db > threshold_db\n",
        "        nonzero = np.nonzero(mask)[0]\n",
        "\n",
        "        if len(nonzero) == 0:\n",
        "            return waveform\n",
        "\n",
        "        # Calculate trim points\n",
        "        start = nonzero[0] * frame_length\n",
        "        end = min((nonzero[-1] + 1) * frame_length, len(audio_np))\n",
        "\n",
        "        # Convert back to torch tensor efficiently\n",
        "        trimmed = torch.from_numpy(audio_np[start:end]).to(waveform.device)\n",
        "        return trimmed.unsqueeze(0) if waveform.dim() == 2 else trimmed\n",
        "\n",
        "    except Exception as e:\n",
        "        log.warning(f\"Silence trimming failed: {str(e)}\")\n",
        "        return waveform\n",
        "\n",
        "\n",
        "def get_main_pitch(audio_data, sr, min_note='C1', max_note='C7'):\n",
        "    \"\"\"Get the main pitch from audio data\"\"\"\n",
        "    try:\n",
        "        # Ensure input is numpy array\n",
        "        if torch.is_tensor(audio_data):\n",
        "            audio_data = audio_data.numpy()\n",
        "\n",
        "        # Calculate pitch using PYIN algorithm\n",
        "        f0, voiced_flag, voiced_probs = librosa.pyin(\n",
        "            audio_data,\n",
        "            fmin=librosa.note_to_hz(min_note),\n",
        "            fmax=librosa.note_to_hz(max_note),\n",
        "            sr=sr,\n",
        "            frame_length=2048,\n",
        "            win_length=1024,\n",
        "            hop_length=512\n",
        "        )\n",
        "\n",
        "        # Filter out unvoiced and low probability segments\n",
        "        mask = voiced_flag & (voiced_probs > 0.6)\n",
        "        f0_valid = f0[mask]\n",
        "\n",
        "        if len(f0_valid) == 0:\n",
        "            log.warning(\"No valid pitch detected\")\n",
        "            return None, None, 0.0\n",
        "\n",
        "        # Get the median frequency\n",
        "        median_f0 = float(np.median(f0_valid))\n",
        "\n",
        "        # Convert to note\n",
        "        closest_note = librosa.hz_to_note(median_f0)\n",
        "        note_freq = librosa.note_to_hz(closest_note)\n",
        "        note = {'closest_note': closest_note, 'freq': note_freq}\n",
        "\n",
        "        # Calculate confidence\n",
        "        confidence = float(np.mean(voiced_probs[voiced_flag]))\n",
        "\n",
        "        return median_f0, note, confidence\n",
        "\n",
        "    except Exception as e:\n",
        "        log.error(f\"Pitch detection failed: {str(e)}\")\n",
        "        raise\n",
        "## ________________ MUCH FASTER BUT ONLY CONSIDERING a Specified time segment of the audio  ________________\n",
        "\n",
        "def detect_pitch_optimized(audio_data, sr, min_note='C1', max_note='C7', analysis_ratio=0.2):\n",
        "    \"\"\"Get the main pitch from audio data\n",
        "\n",
        "    Args:\n",
        "        audio_data: Input audio array\n",
        "        sr: Sample rate\n",
        "        min_note: Minimum note to detect\n",
        "        max_note: Maximum note to detect\n",
        "        analysis_ratio: Ratio of total audio length to analyze (0.0 to 1.0)\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Ensure input is numpy array\n",
        "        if torch.is_tensor(audio_data):\n",
        "            audio_data = audio_data.numpy()\n",
        "\n",
        "        # Downsample for pitch detection if sample rate is high\n",
        "        if sr > 22050:\n",
        "            target_sr = 22050\n",
        "            audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=target_sr)\n",
        "            sr = target_sr\n",
        "\n",
        "        # Calculate segment length based on ratio\n",
        "        total_length = len(audio_data)\n",
        "        segment_len = int(total_length * analysis_ratio)  # Simply take ratio of total length\n",
        "\n",
        "        # Calculate segment boundaries\n",
        "        start_pos = total_length // 8 # Start from 1/8th of audio\n",
        "        end_pos = start_pos + segment_len\n",
        "\n",
        "        # Take segment from middle of audio\n",
        "        audio_segment = audio_data[start_pos:end_pos]\n",
        "\n",
        "        log.info(f\"Analyzing {analysis_ratio*100:.1f}% of audio \"\n",
        "                   f\"({len(audio_segment)/sr:.3f}s out of {total_length/sr:.3f}s total) \"\n",
        "                   f\"from position {start_pos/sr:.3f}s to {end_pos/sr:.3f}s\")\n",
        "\n",
        "\n",
        "        # Calculate pitch using PYIN algorithm with optimized parameters\n",
        "        f0, voiced_flag, voiced_probs = librosa.pyin(\n",
        "            audio_segment,\n",
        "            fmin=librosa.note_to_hz(min_note),\n",
        "            fmax=librosa.note_to_hz(max_note),\n",
        "            sr=sr,\n",
        "            frame_length=1024,  # Reduced from 2048\n",
        "            win_length=512,     # Reduced from 1024\n",
        "            hop_length=256      # Reduced from 512\n",
        "        )\n",
        "\n",
        "        low_prob_ratio = 0.3  # Set a low probability threshold 0.0 to 1.0\n",
        "\n",
        "        # Filter out unvoiced and low probability segments\n",
        "        mask = voiced_flag & (voiced_probs > low_prob_ratio)\n",
        "        f0_valid = f0[mask]\n",
        "\n",
        "        if len(f0_valid) == 0:\n",
        "            log.warning(\"No valid pitch detected\")\n",
        "            return None, None, 0.0\n",
        "\n",
        "        # Get the median frequency\n",
        "        median_f0 = float(np.median(f0_valid))\n",
        "\n",
        "        # Convert to note\n",
        "        closest_note = librosa.hz_to_note(median_f0)\n",
        "        note_freq = librosa.note_to_hz(closest_note)\n",
        "        note = {'closest_note': closest_note, 'freq': note_freq}\n",
        "\n",
        "        # Calculate confidence\n",
        "        confidence = float(np.mean(voiced_probs[voiced_flag]))\n",
        "\n",
        "        return median_f0, note, confidence\n",
        "\n",
        "    except Exception as e:\n",
        "        log.error(f\"Pitch detection failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# Main processing function\n",
        "def process_audio_file(file_path, options):\n",
        "    \"\"\"Process audio file with given options.\"\"\"\n",
        "    start_time = time.time()\n",
        "    results = []\n",
        "\n",
        "    try:\n",
        "        # Load and convert to mono if needed\n",
        "        waveform, sr = torchaudio.load(file_path)\n",
        "        waveform = waveform.to(device)\n",
        "        if waveform.size(0) > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "        # Process based on options\n",
        "        if options['normalize']:\n",
        "            waveform = waveform / torch.max(torch.abs(waveform))\n",
        "\n",
        "        if options['trim']:\n",
        "            waveform = trim_silence(waveform)\n",
        "\n",
        "        if options['tune']:\n",
        "            audio_np = waveform[0].cpu().numpy()\n",
        "            detected_pitch, note, confidence = detect_pitch_optimized(audio_np, sr)\n",
        "\n",
        "            if detected_pitch and confidence > 0.6:\n",
        "                target = options.get('target_pitch', 261.6255)  # C4\n",
        "                factor = get_pitch_factor(detected_pitch, target)\n",
        "\n",
        "                if 0.5 <= factor <= 2.0:\n",
        "                    log.info(f\"{note['closest_note']} → {librosa.hz_to_note(target)}\")\n",
        "                    effects = [[\"speed\", str(factor)], [\"rate\", str(sr)]]\n",
        "                    processed_waveform, processed_sr = torchaudio.sox_effects.apply_effects_tensor(\n",
        "                        waveform.cpu(), sr, effects)\n",
        "                    results.append((\"sox\", processed_waveform, processed_sr))\n",
        "                else:\n",
        "                    log.info(\"Pitch adjustment too large\")\n",
        "            else:\n",
        "                log.info(\"No clear pitch detected\")\n",
        "\n",
        "        if not results:\n",
        "            results.append((\"original\", waveform, sr))\n",
        "\n",
        "        log.info(f\"Done in {time.time() - start_time:.1f}s\")\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        log.info(f\"Error: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    finally:\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# Initialize FastAPI app\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Add CORS middleware\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"], # add my client when deploying\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"], # restrict to needed methods when deploying\n",
        "    allow_headers=[\"*\"],\n",
        "    expose_headers=[\"*\"],\n",
        "    max_age=86400,  # Cache preflight requests for 24 hours\n",
        ")\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"Sample Prepper API running\"}\n",
        "\n",
        "\n",
        "@app.get(\"/process\")\n",
        "async def health_check():\n",
        "    return {\"status\": \"ok\"}\n",
        "\n",
        "# Create thread pool for CPU-intensive tasks\n",
        "thread_pool = ThreadPoolExecutor(max_workers=4)\n",
        "\n",
        "async def run_in_thread(func, *args):\n",
        "    \"\"\"Run CPU-intensive tasks in thread pool\"\"\"\n",
        "    return await asyncio.get_event_loop().run_in_executor(thread_pool, func, *args)\n",
        "\n",
        "\n",
        "\n",
        "@app.post(\"/process\")\n",
        "async def process_audio_endpoint(\n",
        "    file: UploadFile = File(...),\n",
        "    options: str = Form(default=None)\n",
        "):\n",
        "    \"\"\"Process audio file endpoint\"\"\"\n",
        "    try:\n",
        "        process_options = {\n",
        "            'normalize': False,\n",
        "            'trim': False,\n",
        "            'tune': False,\n",
        "            'returnType': 'blob',\n",
        "            'outputFormat': 'wav'\n",
        "        }\n",
        "\n",
        "        if options:\n",
        "            process_options.update(json.loads(options))\n",
        "\n",
        "        # Process file\n",
        "        timestamp = int(time.time() * 1000)\n",
        "        input_path = UPLOAD_DIR / f\"{timestamp}_{file.filename}\"\n",
        "        output_path = None\n",
        "\n",
        "        with open(input_path, \"wb\") as f:\n",
        "            f.write(await file.read())\n",
        "\n",
        "        results = await run_in_thread(process_audio_file, str(input_path), process_options)\n",
        "\n",
        "        if not results:\n",
        "            raise HTTPException(status_code=500, detail=\"Processing failed\")\n",
        "\n",
        "        # Save result\n",
        "        output_path = OUTPUT_DIR / f\"processed_{timestamp}_{file.filename}\"\n",
        "        waveform, sr = results[0][1], results[0][2]\n",
        "\n",
        "        torchaudio.save(\n",
        "            str(output_path),\n",
        "            waveform.cpu(),\n",
        "            sr,\n",
        "            encoding=\"PCM_S\",\n",
        "            bits_per_sample=16\n",
        "        )\n",
        "\n",
        "        return FileResponse(\n",
        "            path=output_path,\n",
        "            filename=f\"processed_{timestamp}_{file.filename}\",\n",
        "            media_type='audio/wav'\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        log.info(f\"Error: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "    finally:\n",
        "        # Cleanup in background task\n",
        "        async def cleanup():\n",
        "            await asyncio.sleep(1)  # Wait for file transfer to complete\n",
        "            if input_path.exists():\n",
        "                input_path.unlink()\n",
        "            if output_path and output_path.exists():\n",
        "                output_path.unlink()\n",
        "\n",
        "        background_tasks = BackgroundTasks()\n",
        "        background_tasks.add_task(cleanup)\n",
        "\n"
      ],
      "metadata": {
        "id": "VzVmF4R8Vk1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Start ngrok\n",
        "ngrok.set_auth_token(userdata.get('NGROK_SECRET'))\n",
        "ngrok_tunnel = ngrok.connect(5001, domain=\"singular-roughy-humane.ngrok-free.app\")\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "\n",
        "# Import and run uvicorn directly\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=5001, host='0.0.0.0', workers=1)"
      ],
      "metadata": {
        "id": "42IqyhNgahhW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}